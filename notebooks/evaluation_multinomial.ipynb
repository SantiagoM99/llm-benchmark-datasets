{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e11126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    accuracy_score, \n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "726b8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(filepath: str) -> dict:\n",
    "    \"\"\"Carga resultados desde un archivo JSON.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calculate_metrics(results: dict, exclude_filtered: bool = True) -> dict:\n",
    "    \"\"\"Calcula métricas de clasificación.\"\"\"\n",
    "    df = pd.DataFrame(results['results'])\n",
    "\n",
    "    y_true = df['expected'].tolist()\n",
    "    y_pred = df['prediction'].tolist()\n",
    "    \n",
    "    metrics = {\n",
    "        'model': results['model'],\n",
    "        'shot_type': results['shot_type'],\n",
    "        'num_samples': len(df),\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_true, y_pred\n",
    "\n",
    "def print_classification_report(y_true, y_pred, model_name: str):\n",
    "    \"\"\"Imprime reporte de clasificación detallado.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Classification Report: {model_name}\")\n",
    "    print('='*60)\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name: str, labels: list = None):\n",
    "    \"\"\"Genera matriz de confusión.\"\"\"\n",
    "    if labels is None:\n",
    "        labels = sorted(set(y_true) | set(y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5cb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('../results/multinomial')\n",
    "\n",
    "# Archivos a evaluar\n",
    "result_files = {\n",
    "    'GPT-4o-mini (few-shot)': results_dir / 'gpt-4o-mini/few_shot_5/gpt-4o-mini_multinomial_few_shot_5_test_20251205_093258.json',\n",
    "    'GPT-4o-mini (zero-shot)': results_dir / 'gpt-4o-mini/zero_shot/gpt-4o-mini_multinomial_zero_shot_test_20251205_094208.json',\n",
    "    'Gemma-2-9B (few-shot)': results_dir / 'google_gemma-2-9b-it/few_shot_5/google_gemma-2-9b-it_multinomial_few_shot_5_test_20251205_111701.json',\n",
    "    'Gemma-2-9B (zero-shot)': results_dir / 'google_gemma-2-9b-it/zero_shot/google_gemma-2-9b-it_multinomial_zero_shot_test_20251205_110744.json',\n",
    "    'Llama-3.1-8B (few-shot)': results_dir / 'meta-llama_Meta-Llama-3.1-8B-Instruct/few_shot_5/meta-llama_Meta-Llama-3.1-8B-Instruct_multinomial_few_shot_5_test_20251205_120135.json',\n",
    "    'Llama-3.1-8B (zero-shot)': results_dir / 'meta-llama_Meta-Llama-3.1-8B-Instruct/zero_shot/meta-llama_Meta-Llama-3.1-8B-Instruct_multinomial_zero_shot_test_20251205_115423.json',\n",
    "    'Qwen-2.5-7B (few-shot)': results_dir / 'Qwen_Qwen2.5-7B-Instruct/few_shot_5/Qwen_Qwen2.5-7B-Instruct_multinomial_few_shot_5_test_20251205_125932.json',\n",
    "    'Qwen-2.5-7B (zero-shot)': results_dir / 'Qwen_Qwen2.5-7B-Instruct/zero_shot/Qwen_Qwen2.5-7B-Instruct_multinomial_zero_shot_test_20251205_125035.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "367bf41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- GPT-4o-mini (few-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.8364\n",
      "  F1 Macro: 0.7858\n",
      "  F1 Micro: 0.8364\n",
      "  F1 Weighted: 0.8413\n",
      "\n",
      "- GPT-4o-mini (zero-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.8228\n",
      "  F1 Macro: 0.6428\n",
      "  F1 Micro: 0.8228\n",
      "  F1 Weighted: 0.8302\n",
      "\n",
      "- Gemma-2-9B (few-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.8117\n",
      "  F1 Macro: 0.4800\n",
      "  F1 Micro: 0.8117\n",
      "  F1 Weighted: 0.8229\n",
      "\n",
      "- Gemma-2-9B (zero-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.7733\n",
      "  F1 Macro: 0.3320\n",
      "  F1 Micro: 0.7733\n",
      "  F1 Weighted: 0.7927\n",
      "\n",
      "- Llama-3.1-8B (few-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.8005\n",
      "  F1 Macro: 0.4737\n",
      "  F1 Micro: 0.8005\n",
      "  F1 Weighted: 0.8178\n",
      "\n",
      "- Llama-3.1-8B (zero-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.7119\n",
      "  F1 Macro: 0.2122\n",
      "  F1 Micro: 0.7119\n",
      "  F1 Weighted: 0.7410\n",
      "\n",
      "- Qwen-2.5-7B (few-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.7446\n",
      "  F1 Macro: 0.5087\n",
      "  F1 Micro: 0.7446\n",
      "  F1 Weighted: 0.7560\n",
      "\n",
      "- Qwen-2.5-7B (zero-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.7606\n",
      "  F1 Macro: 0.3983\n",
      "  F1 Micro: 0.7606\n",
      "  F1 Weighted: 0.7756\n",
      "\n",
      "- Qwen-2.5-7B (zero-shot)\n",
      "  Samples: 1253\n",
      "  Accuracy: 0.7606\n",
      "  F1 Macro: 0.3983\n",
      "  F1 Micro: 0.7606\n",
      "  F1 Weighted: 0.7756\n"
     ]
    }
   ],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for name, filepath in result_files.items():\n",
    "    \n",
    "    results = load_results(filepath)\n",
    "    metrics, y_true, y_pred = calculate_metrics(results)\n",
    "    metrics['name'] = name\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    print(f\"\\n- {name}\")\n",
    "    print(f\"  Samples: {metrics['num_samples']}\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  F1 Micro: {metrics['f1_micro']:.4f}\")\n",
    "    print(f\"  F1 Weighted: {metrics['f1_weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f7f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTADOS COMPARATIVOS\n",
      "================================================================================\n",
      "                    name  num_samples  accuracy  f1_macro  f1_micro  f1_weighted\n",
      "  GPT-4o-mini (few-shot)         1253  0.836393  0.785786  0.836393     0.841306\n",
      " GPT-4o-mini (zero-shot)         1253  0.822825  0.642772  0.822825     0.830182\n",
      "  Qwen-2.5-7B (few-shot)         1253  0.744613  0.508684  0.744613     0.756016\n",
      "   Gemma-2-9B (few-shot)         1253  0.811652  0.480022  0.811652     0.822898\n",
      " Llama-3.1-8B (few-shot)         1253  0.800479  0.473717  0.800479     0.817824\n",
      " Qwen-2.5-7B (zero-shot)         1253  0.760575  0.398305  0.760575     0.775577\n",
      "  Gemma-2-9B (zero-shot)         1253  0.773344  0.331966  0.773344     0.792709\n",
      "Llama-3.1-8B (zero-shot)         1253  0.711891  0.212193  0.711891     0.741009\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.DataFrame(all_metrics)\n",
    "df_metrics = df_metrics[['name', 'num_samples', 'accuracy', 'f1_macro', 'f1_micro', 'f1_weighted']]\n",
    "df_metrics = df_metrics.sort_values('f1_macro', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTADOS COMPARATIVOS\")\n",
    "print(\"=\"*80)\n",
    "print(df_metrics.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-benchmark-datasets (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
