{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75e5b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /\n",
      "Existe src/: False\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2511693244.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Existe src/: {(project_root / 'src').exists()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Ahora importa SIN los \"..\" (imports absolutos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultilabel_predictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLabelPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_multilabel_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultilabel_datareader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    # En algunos entornos de notebook, __file__ no existe\n",
    "    notebook_path = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Si no existe __file__, usar el directorio actual y buscar hacia arriba\n",
    "    notebook_path = Path.cwd()\n",
    "    # Buscar el directorio que contiene 'src'\n",
    "    while not (notebook_path / 'src').exists() and notebook_path != notebook_path.parent:\n",
    "        notebook_path = notebook_path.parent\n",
    "\n",
    "project_root = notebook_path\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Existe src/: {(project_root / 'src').exists()}\")\n",
    "# Ahora importa SIN los \"..\" (imports absolutos)\n",
    "from src.evaluation.multilabel_predictor import MultiLabelPredictor\n",
    "from src.evaluation.metrics import compute_multilabel_metrics, save_metrics\n",
    "from src.utils.multilabel_datareader import MultiLabelDataset\n",
    "from src.utils.jel_categories import get_jel_names\n",
    "from src.models.huggingface_llm import HuggingFaceLLM\n",
    "from src.models.llm_multilabel_model import LLMMultilabelModel\n",
    "from src.prompts.multilabel_prompt import MultiLabelPromptTemplate\n",
    "import torch\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTING MULTILABEL CLASSIFICATION FOR JEL GENERAL CATEGORIES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\n[1/4] Loading dataset...\")\n",
    "    dataset = MultiLabelDataset(data_dir=\"data/multilabel_banrep\")\n",
    "    print(dataset)\n",
    "    print(f\"Available labels: {dataset.labels}\")\n",
    "    \n",
    "    # Create LLM\n",
    "    print(\"\\n[2/4] Loading LLM...\")\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        load_in_4bit=False,\n",
    "        torch_dtype=torch.float32 if not torch.cuda.is_available() else torch.float16\n",
    "    )\n",
    "    \n",
    "    # Create prompt template\n",
    "    print(\"\\n[3/4] Creating prompt template...\")\n",
    "    label_names = get_jel_names(language=\"es\")\n",
    "    prompt_template = MultiLabelPromptTemplate(\n",
    "        available_labels=dataset.labels,\n",
    "        language=\"es\",\n",
    "        label_descriptions=label_names,\n",
    "    )\n",
    "    \n",
    "    # Create multi-label model\n",
    "    model = LLMMultiLabelModel(\n",
    "        llm=llm,\n",
    "        available_labels=dataset.labels,\n",
    "        prompt_template=prompt_template,\n",
    "        batch_size=4\n",
    "    )\n",
    "    print(model)\n",
    "    \n",
    "    # Create predictor and save results for all splits\n",
    "    print(\"\\n[4/4] Generating predictions and computing metrics for splits...\")\n",
    "    predictor = MultiLabelPredictor(model, dataset)\n",
    "\n",
    "    for split in [\"dev\", \"test\"]:\n",
    "        print(f\"\\nProcessing split: {split}\")\n",
    "        results_df = predictor.predict_split(split=split, batch_size=4)\n",
    "\n",
    "        # Save predictions\n",
    "        output_pred = Path(f\"results/multilabel/TinyLlama-1.1B/{split}_predictions.parquet\")\n",
    "        output_pred.parent.mkdir(parents=True, exist_ok=True)\n",
    "        results_df.to_parquet(output_pred, index=False)\n",
    "        print(f\"Predicciones guardadas en: {output_pred}\")\n",
    "\n",
    "        # Compute and save metrics\n",
    "        metrics = compute_multilabel_metrics(\n",
    "            true_labels=results_df[\"true_labels\"].tolist(),\n",
    "            pred_labels=results_df[\"predicted_labels\"].tolist(),\n",
    "            all_labels=dataset.labels,\n",
    "        )\n",
    "        output_metrics = Path(f\"results/multilabel/TinyLlama-1.1B/{split}_metrics.json\")\n",
    "        save_metrics(metrics, str(output_metrics))\n",
    "        print(f\"Métricas guardadas en: {output_metrics}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5301187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "total 16\n",
      "drwxr-xr-x 1 root root 4096 Dec  9 14:41 .\n",
      "drwxr-xr-x 1 root root 4096 Dec 13 07:01 ..\n",
      "drwxr-xr-x 4 root root 4096 Dec  9 14:41 .config\n",
      "drwxr-xr-x 1 root root 4096 Dec  9 14:42 sample_data\n",
      "total 16\n",
      "drwxr-xr-x 1 root root 4096 Dec  9 14:41 .\n",
      "drwxr-xr-x 1 root root 4096 Dec 13 07:01 ..\n",
      "drwxr-xr-x 4 root root 4096 Dec  9 14:41 .config\n",
      "drwxr-xr-x 1 root root 4096 Dec  9 14:42 sample_data\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a819e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: cd: too many arguments\n"
     ]
    }
   ],
   "source": [
    "!cd /content pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0995643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de resultados y datos\n",
    "results_dir = Path('../results/multilabel/TinyLlama-1.1B')\n",
    "data_dir = Path('../data/multilabel_banrep')\n",
    "\n",
    "# Cargar dataset para obtener las etiquetas disponibles\n",
    "dataset = MultiLabelDataset(str(data_dir))\n",
    "all_labels = dataset.labels\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75218950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar predicciones dev/test\n",
    "df_dev = pd.read_parquet(results_dir / 'dev_predictions.parquet')\n",
    "df_test = pd.read_parquet(results_dir / 'test_predictions.parquet')\n",
    "print('Dev samples:', len(df_dev), 'Test samples:', len(df_test))\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas para dev y test\n",
    "metrics_dev = compute_multilabel_metrics(\n",
    "    df_dev['true_labels'].tolist(),\n",
    "    df_dev['predicted_labels'].tolist(),\n",
    "    all_labels\n",
    ")\n",
    "metrics_test = compute_multilabel_metrics(\n",
    "    df_test['true_labels'].tolist(),\n",
    "    df_test['predicted_labels'].tolist(),\n",
    "    all_labels\n",
    ")\n",
    "print('OK: métricas calculadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27595993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de métricas\n",
    "summary = pd.DataFrame([\n",
    "print('\n",
    "' + '='*60)\n",
    "print('RESUMEN DE MÉTRICAS')\n",
    "print('='*60)\n",
    "print(summary.to_string(index=False))\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da579a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas por etiqueta (test) con nombres JEL\n",
    "jel_names = get_jel_names('es')\n",
    "per_label_test_df = pd.DataFrame.from_dict(metrics_test['per_label'], orient='index').reset_index().rename(columns={'index': 'label'})\n",
    "per_label_test_df['name'] = per_label_test_df['label'].map(jel_names)\n",
    "per_label_test_df = per_label_test_df.sort_values(['support','f1'], ascending=[False, False])\n",
    "per_label_test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones: soporte y F1 para top etiquetas\n",
    "top_support = per_label_test_df.head(15)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top_support, x='label', y='support')\n",
    "plt.title('Top 15 etiquetas por soporte (test)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top_support, x='label', y='f1')\n",
    "plt.title('F1 para top 15 etiquetas (test)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df76ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar métricas en JSON (opcional)\n",
    "with open(results_dir / 'dev_metrics_notebook.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_dev, f, ensure_ascii=False, indent=2)\n",
    "with open(results_dir / 'test_metrics_notebook.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_test, f, ensure_ascii=False, indent=2)\n",
    "print('Guardadas métricas dev/test en JSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos y rutas de predicciones para evaluación comparativa\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ajusta estas rutas según tus carpetas en results/multilabel/\n",
    "multilabel_results_root = Path('../results/multilabel')\n",
    "\n",
    "result_files = {\n",
    "    'TinyLlama-1.1B (zero-shot)': multilabel_results_root / 'TinyLlama-1.1B/test_predictions.parquet',\n",
    "    # Ejemplos adicionales si ya generaste predicciones con estos modelos:\n",
    "    # 'Llama-3.1-8B (zero-shot)': multilabel_results_root / 'meta-llama_Meta-Llama-3.1-8B-Instruct/test_predictions.parquet',\n",
    "    # 'Gemma-2-9B (zero-shot)': multilabel_results_root / 'google_gemma-2-9b-it/test_predictions.parquet',\n",
    "    # 'Qwen-2.5-7B (zero-shot)': multilabel_results_root / 'Qwen_Qwen2.5-7B-Instruct/test_predictions.parquet',\n",
    "}\n",
    "\n",
    "# Cargar dataset (si no existe en variables previas) para obtener etiquetas\n",
    "if 'all_labels' not in globals() or not all_labels:\n",
    "    dataset = MultiLabelDataset('../data/multilabel_banrep')\n",
    "    all_labels = dataset.labels\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación comparativa: cargar y calcular métricas para cada modelo\n",
    "from src.evaluation.metrics import compute_multilabel_metrics\n",
    "\n",
    "all_metrics = []\n",
    "failed = []\n",
    "\n",
    "for name, filepath in result_files.items():\n",
    "    if not filepath.exists():\n",
    "        failed.append((name, str(filepath)))\n",
    "        print(f\"[WARN] No existe: {filepath}\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_parquet(filepath)\n",
    "    if not {'true_labels','predicted_labels'}.issubset(df.columns):\n",
    "        failed.append((name, str(filepath)))\n",
    "        print(f\"[WARN] Faltan columnas en {filepath} (true_labels/predicted_labels)\")\n",
    "        continue\n",
    "    \n",
    "    m = compute_multilabel_metrics(\n",
    "        df['true_labels'].tolist(),\n",
    "        df['predicted_labels'].tolist(),\n",
    "        all_labels,\n",
    "    )\n",
    "    all_metrics.append({\n",
    "        'name': name,\n",
    "        'num_samples': len(df),\n",
    "        'subset_accuracy': m['subset_accuracy'],\n",
    "        'hamming_loss': m['hamming_loss'],\n",
    "        'precision_micro': m['precision_micro'],\n",
    "        'recall_micro': m['recall_micro'],\n",
    "        'f1_micro': m['f1_micro'],\n",
    "        'precision_macro': m['precision_macro'],\n",
    "        'recall_macro': m['recall_macro'],\n",
    "        'f1_macro': m['f1_macro'],\n",
    "    })\n",
    "\n",
    "print(f\"\\nEvaluados {len(all_metrics)} modelos. Fallidos: {len(failed)}\")\n",
    "if failed:\n",
    "    print(\"Fallidos:\")\n",
    "    for name, path in failed:\n",
    "        print(f\"- {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa ordenada por F1 Macro\n",
    "if all_metrics:\n",
    "    df_metrics = pd.DataFrame(all_metrics)\n",
    "    df_metrics = df_metrics[['name','num_samples','subset_accuracy','hamming_loss','f1_macro','f1_micro','precision_macro','recall_macro']]\n",
    "    df_metrics = df_metrics.sort_values('f1_macro', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTADOS COMPARATIVOS MULTILABEL (JEL)\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_metrics.to_string(index=False))\n",
    "else:\n",
    "    print('[WARN] No hay métricas para mostrar. Verifica rutas en result_files.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
