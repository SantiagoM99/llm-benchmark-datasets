{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e11126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea142a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(filepath: str) -> dict:\n",
    "    \"\"\"Carga resultados desde un archivo JSON.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calculate_metrics(results: dict) -> dict:\n",
    "    \"\"\"Calcula mÃ©tricas de NER.\"\"\"\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    for sample in results['results']:\n",
    "        expected = {(e['text'].lower(), e['type'].lower()) for e in sample['expected_entities']}\n",
    "        predicted = {(e['text'].lower(), e['type'].lower()) for e in sample['predicted_entities']}\n",
    "        \n",
    "        tp += len(expected & predicted)\n",
    "        fp += len(predicted - expected)\n",
    "        fn += len(expected - predicted)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_micro = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'model': results['model'],\n",
    "        'shot_type': results['shot_type'],\n",
    "        'num_samples': results['num_samples'],\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebfdd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('../results/ner')\n",
    "\n",
    "result_files = {\n",
    "    'GPT-4o-mini (few-shot)': results_dir / 'gpt-4o-mini/few_shot_5/gpt-4o-mini_ner_few_shot_5_test_20251212_135905.json',\n",
    "    'GPT-4o-mini (zero-shot)': results_dir / 'gpt-4o-mini/zero_shot/gpt-4o-mini_ner_zero_shot_test_20251212_130743.json',\n",
    "    'GPT-5.1 (few-shot)': results_dir / 'gpt-5.1-grande/few_shot_5/gpt-5.1-grande_ner_few_shot_5_test_20251222_125130.json',\n",
    "    'GPT-5.1 (zero-shot)': results_dir / 'gpt-5.1-grande/zero_shot/gpt-5.1-grande_ner_zero_shot_test_20251222_124732.json',\n",
    "    'Gemma-2-9B (few-shot)': results_dir / 'google_gemma-2-9b-it/few_shot_5/google_gemma-2-9b-it_ner_few_shot_5_test_20251215_125935.json',\n",
    "    'Gemma-2-9B (zero-shot)': results_dir / 'google_gemma-2-9b-it/zero_shot/google_gemma-2-9b-it_ner_zero_shot_test_20251215_151733.json',\n",
    "    'Llama-3.1-8B (few-shot)': results_dir / 'meta-llama_Meta-Llama-3.1-8B-Instruct/few_shot_5/meta-llama_Meta-Llama-3.1-8B-Instruct_ner_few_shot_5_test_20251215_174710.json',\n",
    "    'Llama-3.1-8B (zero-shot)': results_dir / 'meta-llama_Meta-Llama-3.1-8B-Instruct/zero_shot/meta-llama_Meta-Llama-3.1-8B-Instruct_ner_zero_shot_test_20251215_193047.json',\n",
    "    'Qwen-2.5-7B (few-shot)': results_dir / 'Qwen_Qwen2.5-7B-Instruct/few_shot_5/Qwen_Qwen2.5-7B-Instruct_ner_few_shot_5_test_20251215_104744.json',\n",
    "    'Qwen-2.5-7B (zero-shot)': results_dir / 'Qwen_Qwen2.5-7B-Instruct/zero_shot/Qwen_Qwen2.5-7B-Instruct_ner_zero_shot_test_20251215_103242.json',\n",
    "    'Mistral-7B (few-shot)': results_dir / 'mistralai_Mistral-7B-Instruct-v0.3/few_shot_5/mistralai_Mistral-7B-Instruct-v0.3_ner_few_shot_5_test_20260103_072628.json',\n",
    "    'Mistral-7B (zero-shot)': results_dir / 'mistralai_Mistral-7B-Instruct-v0.3/zero_shot/mistralai_Mistral-7B-Instruct-v0.3_ner_zero_shot_test_20260103_084221.json',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56729c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- GPT-4o-mini (few-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.2913\n",
      "  Recall: 0.2998\n",
      "  F1 Micro: 0.2955\n",
      "\n",
      "- GPT-4o-mini (zero-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.2455\n",
      "  Recall: 0.2263\n",
      "  F1 Micro: 0.2355\n",
      "\n",
      "- GPT-5.1 (few-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1 Micro: 0.0000\n",
      "\n",
      "- GPT-5.1 (zero-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1 Micro: 0.0000\n",
      "\n",
      "- Gemma-2-9B (few-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.2483\n",
      "  Recall: 0.3284\n",
      "  F1 Micro: 0.2828\n",
      "\n",
      "- Gemma-2-9B (zero-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.2298\n",
      "  Recall: 0.3229\n",
      "  F1 Micro: 0.2685\n",
      "\n",
      "- Llama-3.1-8B (few-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.2255\n",
      "  Recall: 0.3378\n",
      "  F1 Micro: 0.2704\n",
      "\n",
      "- Llama-3.1-8B (zero-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.1556\n",
      "  Recall: 0.1705\n",
      "  F1 Micro: 0.1627\n",
      "\n",
      "- Qwen-2.5-7B (few-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.2884\n",
      "  Recall: 0.2835\n",
      "  F1 Micro: 0.2859\n",
      "\n",
      "- Qwen-2.5-7B (zero-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.2549\n",
      "  Recall: 0.2011\n",
      "  F1 Micro: 0.2248\n",
      "\n",
      "- Mistral-7B (few-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.2416\n",
      "  Recall: 0.3166\n",
      "  F1 Micro: 0.2741\n",
      "\n",
      "- Mistral-7B (zero-shot)\n",
      "  Samples: 1129\n",
      "  Precision: 0.1742\n",
      "  Recall: 0.3172\n",
      "  F1 Micro: 0.2249\n"
     ]
    }
   ],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for name, filepath in result_files.items():\n",
    "    \n",
    "    results = load_results(filepath)\n",
    "    metrics = calculate_metrics(results)\n",
    "    metrics['name'] = name\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    print(f\"\\n- {name}\")\n",
    "    print(f\"  Samples: {metrics['num_samples']}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Micro: {metrics['f1_micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29330e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTADOS COMPARATIVOS NER\n",
      "================================================================================\n",
      "                    name  num_samples  precision   recall  f1_micro\n",
      "  GPT-4o-mini (few-shot)         1129   0.291273 0.299771  0.295461\n",
      "  Qwen-2.5-7B (few-shot)         1129   0.288417 0.283467  0.285920\n",
      "   Gemma-2-9B (few-shot)         1129   0.248324 0.328375  0.282793\n",
      "   Mistral-7B (few-shot)         1129   0.241598 0.316648  0.274078\n",
      " Llama-3.1-8B (few-shot)         1129   0.225468 0.337815  0.270437\n",
      "  Gemma-2-9B (zero-shot)         1129   0.229752 0.322941  0.268490\n",
      " GPT-4o-mini (zero-shot)         1129   0.245500 0.226259  0.235487\n",
      "  Mistral-7B (zero-shot)         1129   0.174234 0.317220  0.224926\n",
      " Qwen-2.5-7B (zero-shot)         1129   0.254895 0.201087  0.224816\n",
      "Llama-3.1-8B (zero-shot)         1129   0.155573 0.170481  0.162686\n",
      "     GPT-5.1 (zero-shot)         1129   0.000000 0.000000  0.000000\n",
      "      GPT-5.1 (few-shot)         1129   0.000000 0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.DataFrame(all_metrics)\n",
    "df_metrics = df_metrics[['name', 'num_samples', 'precision', 'recall', 'f1_micro']]\n",
    "df_metrics = df_metrics.sort_values('f1_micro', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTADOS COMPARATIVOS NER\")\n",
    "print(\"=\"*80)\n",
    "print(df_metrics.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-benchmark-datasets (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
